name: Daddylive Resolver

on:
  workflow_dispatch:
    inputs:
      channel_urls:
        description: "Comma-separated list of Daddylive channel or player2 iframe URLs"
        required: true
        default: "https://example.com/your-channel-page"
      max_channels:
        description: "Limit channels to process (0 = all)"
        required: false
        default: "0"
      save_to_db:
        description: "Upsert resolved streams to Supabase? (0/1)"
        required: false
        default: "0"
  schedule:
    - cron: "0 8 * * *" # daily 08:00 UTC (optional)

permissions:
  contents: read

concurrency:
  group: daddylive-resolver
  cancel-in-progress: true

jobs:
  resolve:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    defaults:
      run:
        shell: bash
    env:
      DL_XML_URL: "https://raw.githubusercontent.com/thecrewwh/dl_url/refs/heads/main/dl.xml"
      USER_AGENT: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"
      PROBE_TIMEOUT_S: "12"
      PROBE_VARIANT_LIMIT: "6"   # donâ€™t slurp huge playlists
      # Supabase (optional upsert)
      SUPABASE_URL: "${{ secrets.SUPABASE_URL }}"
      SUPABASE_SERVICE_KEY: "${{ secrets.SUPABASE_SERVICE_KEY }}"
      SUPABASE_SCHEMA: "public"
      STREAMS_TABLE: "streams_latam"   # or mx_working_streams if you prefer

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Write resolver script
        run: |
          set -euo pipefail
          mkdir -p scripts out/daddylive
          cat > scripts/daddylive_resolve.py <<'PY'
          import os, re, io, sys, json, time, base64, csv
          from urllib.parse import urljoin, urlparse
          import requests

          DL_XML_URL = os.environ.get("DL_XML_URL")
          UA = os.environ.get("USER_AGENT","Mozilla/5.0")
          PROBE_TIMEOUT = int(os.environ.get("PROBE_TIMEOUT_S","12"))
          VARIANT_LIMIT = int(os.environ.get("PROBE_VARIANT_LIMIT","6"))
          SAVE_TO_DB = os.environ.get("save_to_db","0") in ("1","true","True")

          SUPABASE_URL = os.environ.get("SUPABASE_URL","").strip()
          SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY","").strip()
          SUPABASE_SCHEMA = os.environ.get("SUPABASE_SCHEMA","public")
          STREAMS_TABLE = os.environ.get("STREAMS_TABLE","streams_latam")

          CHANNEL_URLS = [u.strip() for u in (os.environ.get("channel_urls","").strip() or "").split(",") if u.strip()]
          try:
              MAX_CHANNELS = int(os.environ.get("max_channels","0"))
          except:
              MAX_CHANNELS = 0
          if MAX_CHANNELS and len(CHANNEL_URLS) > MAX_CHANNELS:
              CHANNEL_URLS = CHANNEL_URLS[:MAX_CHANNELS]

          OUT_DIR = "out/daddylive"
          os.makedirs(OUT_DIR, exist_ok=True)
          REPORT_JSONL = os.path.join(OUT_DIR, "resolved.jsonl")
          REPORT_CSV = os.path.join(OUT_DIR, "resolved.csv")

          S = requests.Session()
          S.headers.update({"User-Agent": UA, "Connection":"keep-alive"})

          def fetch_dl_xml():
              r = S.get(DL_XML_URL, timeout=20)
              r.raise_for_status()
              return r.text

          def extract_baseurl(xml_text):
              # look for src="https://<base>/" (first one wins)
              m = re.search(r'src=["\'](https?://[^"\']+/)["\']', xml_text, re.IGNORECASE)
              return m.group(1).rstrip("/") if m else None

          def get_html(url, referer=None, origin=None):
              headers = {}
              if referer: headers["Referer"] = referer
              if origin:  headers["Origin"]  = origin
              r = S.get(url, headers=headers, timeout=20)
              r.raise_for_status()
              return r.text, r.url

          def find_iframe_url(html, base):
              # Prefer player2 iframe; fallback to any iframe with /player or /cast
              cands = re.findall(r'<iframe[^>]+src=["\']([^"\']+)["\']', html, re.IGNORECASE)
              best = None
              for u in cands:
                  full = urljoin(base, u)
                  if "player2" in full or "player" in full or "/cast/" in full:
                      best = full
                      break
              return urljoin(base, best) if best else None

          def extract_channel_key(html):
              # const CHANNEL_KEY = "<...>";
              m = re.search(r'CHANNEL_KEY\s*=\s*["\']([^"\']+)["\']', html)
              return m.group(1) if m else None

          def extract_server_lookup(html):
              # fish for a path like /api/server?ch=  OR any path with 'server' that looks like an endpoint
              m = re.search(r'["\'](\/[a-zA-Z0-9_\/\-\.\?\=\&]*server[a-zA-Z0-9_\/\-\.\?\=\&]*)["\']', html)
              if m:
                  return m.group(1)
              # fallback: sometimes embedded JSON contains it; try looser search
              m = re.search(r'(\/[a-zA-Z0-9_\/\-\.\?\=\&]*getserver[a-zA-Z0-9_\/\-\.\?\=\&]*)', html, re.IGNORECASE)
              return m.group(1) if m else None

          def call_server_lookup(iframe_url, server_lookup_path, channel_key, base_referer):
              origin = "{uri.scheme}://{uri.netloc}".format(uri=urlparse(iframe_url))
              referer = origin + "/"
              url = urljoin(origin, server_lookup_path)
              if channel_key and "ch=" in url:
                  # already has ch=
                  pass
              elif channel_key:
                  if url.endswith("?") or url.endswith("&"):
                      url = f"{url}ch={channel_key}"
                  elif "?" in url:
                      url = f"{url}&ch={channel_key}"
                  else:
                      url = f"{url}?ch={channel_key}"
              r = S.get(url, headers={"Referer":referer, "Origin":origin, "User-Agent":UA}, timeout=20)
              # Expect JSON like {"server_key":"top1/cdn"} or {"server_key":"cdn5"}
              try:
                  data = r.json()
              except:
                  data = {}
              return data.get("server_key")

          def maybe_prime_auth(iframe_url, channel_key):
              # Try /auth.php?channel_id=...
              origin = "{uri.scheme}://{uri.netloc}".format(uri=urlparse(iframe_url))
              auth_url = urljoin(origin, f"/auth.php?channel_id={channel_key}")
              try:
                  S.get(auth_url, headers={"Referer": origin+"/", "Origin": origin, "User-Agent": UA}, timeout=15)
              except:
                  pass

          def build_candidates(server_key, channel_key):
              cands = []
              # Template A: top1/cdn special
              if server_key == "top1/cdn":
                  cands.append(f"https://top1.newkso.ru/top1/cdn/{channel_key}/mono.m3u8")
              # Template B: generic
              if server_key:
                  cands.append(f"https://{server_key}new.newkso.ru/{server_key}/{channel_key}/mono.m3u8")
              # Fallback (if lookup failed, still try common top1)
              if not cands:
                  cands.append(f"https://top1.newkso.ru/top1/cdn/{channel_key}/mono.m3u8")
              return cands

          def fetch_text(url, referer, origin, timeout=PROBE_TIMEOUT):
              r = S.get(url, headers={"Referer":referer, "Origin":origin, "User-Agent":UA, "Accept":"application/vnd.apple.mpegurl,text/plain,*/*"}, timeout=timeout, allow_redirects=True)
              ok = (r.status_code == 200)
              return ok, r.text, r.url, dict(r.headers), r.status_code

          def is_master_playlist(txt):
              # Master has #EXT-X-STREAM-INF
              return "#EXTM3U" in txt and "EXT-X-STREAM-INF" in txt

          def first_n_variants(txt, limit=VARIANT_LIMIT):
              out = []
              for line in txt.splitlines():
                  line = line.strip()
                  if not line or line.startswith("#"): 
                      continue
                  out.append(line)
                  if len(out) >= limit:
                      break
              return out

          def resolve_one(channel_or_iframe, baseurl):
              # Step 1: load channel page (if not iframe), then pick player2 iframe
              ref = baseurl + "/"
              if "/player" in channel_or_iframe or "/cast/" in channel_or_iframe:
                  iframe_url = channel_or_iframe
                  iframe_html, iframe_final = get_html(iframe_url, referer=ref, origin=baseurl)
              else:
                  # channel page
                  ch_html, ch_final = get_html(channel_or_iframe, referer=ref, origin=baseurl)
                  iframe_url = find_iframe_url(ch_html, ch_final)
                  if not iframe_url:
                      return {"input": channel_or_iframe, "error":"no_iframe_found"}
                  iframe_html, iframe_final = get_html(iframe_url, referer=ref, origin=baseurl)

              # Step 2: extract keys
              channel_key = extract_channel_key(iframe_html)
              if not channel_key:
                  return {"input": channel_or_iframe, "iframe": iframe_url, "error":"no_channel_key"}

              server_lookup = extract_server_lookup(iframe_html)
              server_key = None
              if server_lookup:
                  server_key = call_server_lookup(iframe_url, server_lookup, channel_key, baseurl)

              # Step 3: optionally prime auth cookie
              maybe_prime_auth(iframe_url, channel_key)

              # Step 4: build candidate m3u8s and probe
              origin = "{uri.scheme}://{uri.netloc}".format(uri=urlparse(iframe_url))
              referer = origin + "/"
              candidates = build_candidates(server_key, channel_key)
              results = []
              for u in candidates:
                  ok, body, final_url, headers, code = fetch_text(u, referer=referer, origin=origin)
                  is_master = ok and is_master_playlist(body)
                  res = {
                      "candidate": u,
                      "http_ok": ok,
                      "status": code,
                      "final_url": final_url,
                      "is_master": is_master,
                      "host": urlparse(final_url).netloc if final_url else None
                  }
                  # quick peek: grab a few variants if master
                  if is_master:
                      res["variants"] = [
                          urljoin(final_url, v) for v in first_n_variants(body, VARIANT_LIMIT)
                      ]
                  results.append(res)

              best = None
              for r in results:
                  if r.get("is_master") and r.get("http_ok"):
                      best = r; break
              if not best:
                  # fallback: first 200
                  for r in results:
                      if r.get("http_ok"):
                          best = r; break

              return {
                  "input": channel_or_iframe,
                  "iframe_url": iframe_url,
                  "channel_key": channel_key,
                  "server_key": server_key,
                  "candidates": results,
                  "best": best
              }

          def upsert_supabase(rows):
              if not SAVE_TO_DB:
                  return
              if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
                  print("No Supabase creds; skipping DB upsert", file=sys.stderr); return
              try:
                  import requests as rq
              except:
                  return
              # Build payload: keep a single record per working candidate
              payload = []
              now_iso = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
              for r in rows:
                  b = r.get("best") or {}
                  if not b.get("final_url"): 
                      continue
                  payload.append({
                      "stream_url": b["final_url"],
                      "channel_name": None,         # unknown here (keep null; you will map later)
                      "channel_id": r.get("channel_key"),
                      "logo_url": None,
                      "source_page_url": r.get("iframe_url"),
                      "extras": {"source":"daddylive","server_key":r.get("server_key")},
                      "working": True if b.get("http_ok") else False,
                      "checked_at": now_iso
                  })
              if not payload:
                  return
              # REST upsert
              url = f"{SUPABASE_URL}/rest/v1/{STREAMS_TABLE}"
              headers = {
                  "apikey": SUPABASE_SERVICE_KEY,
                  "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                  "Content-Type": "application/json",
                  "Prefer": "resolution=merge-duplicates"
              }
              # Upsert on stream_url if your table has unique constraint; otherwise plain insert
              # If you want to enforce upsert on a col, set the unique constraint in DB (recommended).
              r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=40)
              if r.status_code >= 300:
                  print(f"Supabase upsert error: HTTP {r.status_code} {r.text}", file=sys.stderr)

          def main():
              if not CHANNEL_URLS:
                  print("No channel_urls provided.", file=sys.stderr); sys.exit(1)
              xml = fetch_dl_xml()
              baseurl = extract_baseurl(xml)
              if not baseurl:
                  print("Could not extract baseurl from dl.xml", file=sys.stderr); sys.exit(2)

              results = []
              for u in CHANNEL_URLS:
                  try:
                      res = resolve_one(u, baseurl)
                  except Exception as e:
                      res = {"input": u, "error": f"{type(e).__name__}: {e}"}
                  results.append(res)

              # Write JSONL + CSV
              with open(REPORT_JSONL,"w",encoding="utf-8") as jf:
                  for r in results:
                      jf.write(json.dumps(r, ensure_ascii=False) + "\n")

              with open(REPORT_CSV,"w",newline="",encoding="utf-8") as cf:
                  w = csv.writer(cf)
                  w.writerow(["input","final_url","host","status","is_master","server_key","channel_key"])
                  for r in results:
                      b = r.get("best") or {}
                      w.writerow([
                          r.get("input",""),
                          (b.get("final_url") or ""),
                          (b.get("host") or ""),
                          (b.get("status") or ""),
                          ("yes" if b.get("is_master") else ""),
                          (r.get("server_key") or ""),
                          (r.get("channel_key") or "")
                      ])

              upsert_supabase(results)
              print(f"Resolved {len(results)} items. Reports in out/daddylive/", file=sys.stderr)

          if __name__ == "__main__":
              main()
          PY

      - name: Run resolver
        env:
          channel_urls: "${{ github.event.inputs.channel_urls }}"
          max_channels: "${{ github.event.inputs.max_channels }}"
          save_to_db: "${{ github.event.inputs.save_to_db }}"
        run: |
          set -euo pipefail
          python3 scripts/daddylive_resolve.py

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: daddylive-resolved
          path: |
            out/daddylive/resolved.jsonl
            out/daddylive/resolved.csv
          if-no-files-found: error

name: Daddylive Resolver

on:
  workflow_dispatch:
    inputs:
      channel_urls:
        description: "Comma OR newline-separated Daddylive channel or player2 iframe URLs"
        required: false
        default: ""
      max_channels:
        description: "Limit channels to process (0 = all)"
        required: false
        default: "0"
      save_to_db:
        description: "Upsert resolved streams to Supabase? (0/1)"
        required: false
        default: "0"
  schedule:
    - cron: "0 8 * * *"  # optional

permissions:
  contents: read

concurrency:
  group: daddylive-resolver
  cancel-in-progress: true

jobs:
  resolve:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    defaults:
      run:
        shell: bash
    env:
      DL_XML_URL: "https://raw.githubusercontent.com/thecrewwh/dl_url/refs/heads/main/dl.xml"
      USER_AGENT: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"
      PROBE_TIMEOUT_S: "12"
      PROBE_VARIANT_LIMIT: "6"
      SUPABASE_URL: "${{ secrets.SUPABASE_URL }}"
      SUPABASE_SERVICE_KEY: "${{ secrets.SUPABASE_SERVICE_KEY }}"
      SUPABASE_SCHEMA: "public"
      STREAMS_TABLE: "streams_latam"
      # Fallbacks for scheduled runs (optional):
      DADDYLIVE_CHANNEL_URLS: "${{ secrets.DADDYLIVE_CHANNEL_URLS }}"
      DADDYLIVE_CHANNEL_FILE: ""   # e.g., channels.txt in repo (leave blank if unused)

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install requests

      - name: Write resolver script
        run: |
          set -euo pipefail
          mkdir -p scripts out/daddylive
          cat > scripts/daddylive_resolve.py <<'PY'
          import os, re, sys, json, time, csv, base64
          from urllib.parse import urljoin, urlparse
          import requests

          # ---- ENV / CONFIG ----
          DL_XML_URL = os.environ.get("DL_XML_URL")
          UA = os.environ.get("USER_AGENT","Mozilla/5.0")
          PROBE_TIMEOUT = int(os.environ.get("PROBE_TIMEOUT_S","12"))
          VARIANT_LIMIT = int(os.environ.get("PROBE_VARIANT_LIMIT","6"))
          SAVE_TO_DB = os.environ.get("save_to_db","0") in ("1","true","True")

          SUPABASE_URL = os.environ.get("SUPABASE_URL","").strip()
          SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY","").strip()
          SUPABASE_SCHEMA = os.environ.get("SUPABASE_SCHEMA","public")
          STREAMS_TABLE = os.environ.get("STREAMS_TABLE","streams_latam")

          RAW_INPUT = os.environ.get("channel_urls","") or ""
          RAW_SECRET = os.environ.get("DADDYLIVE_CHANNEL_URLS","") or ""
          RAW_FILE = os.environ.get("DADDYLIVE_CHANNEL_FILE","") or ""

          OUT_DIR = "out/daddylive"
          os.makedirs(OUT_DIR, exist_ok=True)
          REPORT_JSONL = os.path.join(OUT_DIR, "resolved.jsonl")
          REPORT_CSV = os.path.join(OUT_DIR, "resolved.csv")

          try:
              MAX_CHANNELS = int(os.environ.get("max_channels","0"))
          except:
              MAX_CHANNELS = 0

          S = requests.Session()
          S.headers.update({"User-Agent": UA, "Connection": "keep-alive"})

          # ---- Utils ----
          def parse_list(raw: str):
              if not raw.strip():
                  return []
              # split on commas and newlines
              parts = []
              for line in raw.replace("\r","").split("\n"):
                  for p in line.split(","):
                      p = p.strip()
                      if p:
                          parts.append(p)
              return parts

          def load_channel_list():
              urls = parse_list(RAW_INPUT)
              if not urls and RAW_SECRET:
                  urls = parse_list(RAW_SECRET)
              if not urls and RAW_FILE:
                  try:
                      with open(RAW_FILE, "r", encoding="utf-8") as f:
                          urls = parse_list(f.read())
                  except FileNotFoundError:
                      urls = []
              if MAX_CHANNELS and len(urls) > MAX_CHANNELS:
                  urls = urls[:MAX_CHANNELS]
              return urls

          def fetch_dl_xml():
              r = S.get(DL_XML_URL, timeout=20)
              r.raise_for_status()
              return r.text

          def extract_baseurl(xml_text):
              m = re.search(r'src=["\'](https?://[^"\']+/)["\']', xml_text, re.IGNORECASE)
              return m.group(1).rstrip("/") if m else None

          def get_html(url, referer=None, origin=None):
              headers = {}
              if referer: headers["Referer"] = referer
              if origin:  headers["Origin"]  = origin
              r = S.get(url, headers=headers, timeout=20, allow_redirects=True)
              r.raise_for_status()
              return r.text, r.url

          def find_iframe_url(html, base):
              cands = re.findall(r'<iframe[^>]+src=["\']([^"\']+)["\']', html, re.IGNORECASE)
              best = None
              for u in cands:
                  full = urljoin(base, u)
                  if "player2" in full or "player" in full or "/cast/" in full:
                      best = full
                      break
              return urljoin(base, best) if best else None

          def extract_channel_key(html):
              m = re.search(r'CHANNEL_KEY\s*=\s*["\']([^"\']+)["\']', html)
              return m.group(1) if m else None

          def extract_server_lookup(html):
              m = re.search(r'["\'](\/[a-zA-Z0-9_\/\-\.\?\=\&]*server[a-zA-Z0-9_\/\-\.\?\=\&]*)["\']', html)
              if m: return m.group(1)
              m = re.search(r'(\/[a-zA-Z0-9_\/\-\.\?\=\&]*getserver[a-zA-Z0-9_\/\-\.\?\=\&]*)', html, re.IGNORECASE)
              return m.group(1) if m else None

          def call_server_lookup(iframe_url, server_lookup_path, channel_key):
              origin = "{u.scheme}://{u.netloc}".format(u=urlparse(iframe_url))
              referer = origin + "/"
              url = urljoin(origin, server_lookup_path)
              if channel_key and "ch=" in url:
                  pass
              elif channel_key:
                  if url.endswith("?") or url.endswith("&"):
                      url = f"{url}ch={channel_key}"
                  elif "?" in url:
                      url = f"{url}&ch={channel_key}"
                  else:
                      url = f"{url}?ch={channel_key}"
              r = S.get(url, headers={"Referer":referer, "Origin":origin, "User-Agent":UA}, timeout=20)
              try:
                  data = r.json()
              except:
                  data = {}
              return data.get("server_key")

          def maybe_prime_auth(iframe_url, channel_key):
              origin = "{u.scheme}://{u.netloc}".format(u=urlparse(iframe_url))
              auth_url = urljoin(origin, f"/auth.php?channel_id={channel_key}")
              try:
                  S.get(auth_url, headers={"Referer": origin+"/", "Origin": origin, "User-Agent": UA}, timeout=15)
              except:
                  pass

          def build_candidates(server_key, channel_key):
              cands = []
              if server_key == "top1/cdn":
                  cands.append(f"https://top1.newkso.ru/top1/cdn/{channel_key}/mono.m3u8")
              if server_key:
                  cands.append(f"https://{server_key}new.newkso.ru/{server_key}/{channel_key}/mono.m3u8")
              if not cands:
                  cands.append(f"https://top1.newkso.ru/top1/cdn/{channel_key}/mono.m3u8")
              return cands

          def fetch_text(url, referer, origin, timeout=PROBE_TIMEOUT):
              r = S.get(url, headers={
                  "Referer":referer, "Origin":origin, "User-Agent":UA,
                  "Accept":"application/vnd.apple.mpegurl,text/plain,*/*"
              }, timeout=timeout, allow_redirects=True)
              ok = (r.status_code == 200)
              return ok, r.text, r.url, dict(r.headers), r.status_code

          def is_master_playlist(txt):
              return "#EXTM3U" in txt and "EXT-X-STREAM-INF" in txt

          def first_n_variants(txt, limit=VARIANT_LIMIT):
              out = []
              for line in txt.splitlines():
                  line = line.strip()
                  if not line or line.startswith("#"):
                      continue
                  out.append(line)
                  if len(out) >= limit:
                      break
              return out

          def resolve_one(channel_or_iframe, baseurl):
              ref = baseurl + "/"
              try:
                  if "/player" in channel_or_iframe or "/cast/" in channel_or_iframe:
                      iframe_url = channel_or_iframe
                      iframe_html, iframe_final = get_html(iframe_url, referer=ref, origin=baseurl)
                  else:
                      ch_html, ch_final = get_html(channel_or_iframe, referer=ref, origin=baseurl)
                      iframe_url = find_iframe_url(ch_html, ch_final)
                      if not iframe_url:
                          return {"input": channel_or_iframe, "error":"no_iframe_found"}
                      iframe_html, iframe_final = get_html(iframe_url, referer=ref, origin=baseurl)

                  channel_key = extract_channel_key(iframe_html)
                  if not channel_key:
                      return {"input": channel_or_iframe, "iframe_url": iframe_url, "error":"no_channel_key"}

                  server_lookup = extract_server_lookup(iframe_html)
                  server_key = call_server_lookup(iframe_url, server_lookup, channel_key) if server_lookup else None
                  maybe_prime_auth(iframe_url, channel_key)

                  origin = "{u.scheme}://{u.netloc}".format(u=urlparse(iframe_url))
                  referer = origin + "/"
                  candidates = build_candidates(server_key, channel_key)
                  results = []
                  for u in candidates:
                      ok, body, final_url, headers, code = fetch_text(u, referer=referer, origin=origin)
                      is_master = ok and is_master_playlist(body)
                      res = {
                          "candidate": u,
                          "http_ok": ok,
                          "status": code,
                          "final_url": final_url,
                          "is_master": is_master,
                          "host": urlparse(final_url).netloc if final_url else None
                      }
                      if is_master:
                          res["variants"] = [ urljoin(final_url, v) for v in first_n_variants(body, VARIANT_LIMIT) ]
                      results.append(res)

                  best = None
                  for r in results:
                      if r.get("is_master") and r.get("http_ok"):
                          best = r; break
                  if not best:
                      for r in results:
                          if r.get("http_ok"):
                              best = r; break

                  return {
                      "input": channel_or_iframe,
                      "iframe_url": iframe_url,
                      "channel_key": channel_key,
                      "server_key": server_key,
                      "candidates": results,
                      "best": best
                  }
              except Exception as e:
                  return {"input": channel_or_iframe, "error": f"{type(e).__name__}: {e}"}

          def upsert_supabase(rows):
              if not SAVE_TO_DB:
                  return
              if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
                  print("No Supabase creds; skipping DB upsert", file=sys.stderr); return
              payload = []
              now_iso = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
              for r in rows:
                  b = (r.get("best") or {})
                  if not b.get("final_url"):
                      continue
                  payload.append({
                      "stream_url": b["final_url"],
                      "channel_name": None,
                      "channel_id": r.get("channel_key"),
                      "logo_url": None,
                      "source_page_url": r.get("iframe_url"),
                      "extras": {"source":"daddylive","server_key":r.get("server_key")},
                      "working": True if b.get("http_ok") else False,
                      "checked_at": now_iso
                  })
              if not payload:
                  return
              url = f"{SUPABASE_URL}/rest/v1/{STREAMS_TABLE}"
              headers = {
                  "apikey": SUPABASE_SERVICE_KEY,
                  "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                  "Content-Type": "application/json",
                  "Prefer": "resolution=merge-duplicates"
              }
              r = S.post(url, headers=headers, data=json.dumps(payload), timeout=40)
              if r.status_code >= 300:
                  print(f"Supabase upsert error: HTTP {r.status_code} {r.text}", file=sys.stderr)

          def write_reports(results):
              # JSONL
              with open(REPORT_JSONL,"w",encoding="utf-8") as jf:
                  for r in results:
                      jf.write(json.dumps(r, ensure_ascii=False) + "\n")
              # CSV
              with open(REPORT_CSV,"w",newline="",encoding="utf-8") as cf:
                  w = csv.writer(cf)
                  w.writerow(["input","final_url","host","status","is_master","server_key","channel_key","error"])
                  for r in results:
                      b = r.get("best") or {}
                      w.writerow([
                          r.get("input",""),
                          (b.get("final_url") or ""),
                          (b.get("host") or ""),
                          (b.get("status") or ""),
                          ("yes" if b.get("is_master") else ""),
                          (r.get("server_key") or ""),
                          (r.get("channel_key") or ""),
                          (r.get("error") or "")
                      ])

          def main():
              urls = load_channel_list()
              results = []

              if not urls:
                  # still write empty reports so artifact upload succeeds
                  write_reports([])
                  print("No channel URLs provided. Set workflow input, secret DADDYLIVE_CHANNEL_URLS, or file path DADDYLIVE_CHANNEL_FILE.", file=sys.stderr)
                  return

              xml = fetch_dl_xml()
              baseurl = extract_baseurl(xml)
              if not baseurl:
                  write_reports([])
                  print("Could not extract baseurl from dl.xml", file=sys.stderr)
                  return

              for u in urls:
                  res = resolve_one(u, baseurl)
                  results.append(res)

              write_reports(results)
              upsert_supabase(results)
              print(f"Resolved {len(results)} items. Reports in out/daddylive/", file=sys.stderr)

          if __name__ == "__main__":
              main()
          PY

      - name: Run resolver
        env:
          channel_urls: "${{ github.event.inputs.channel_urls }}"
          max_channels: "${{ github.event.inputs.max_channels }}"
          save_to_db: "${{ github.event.inputs.save_to_db }}"
        run: |
          set -euo pipefail
          python3 scripts/daddylive_resolve.py

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: daddylive-resolved
          path: |
            out/daddylive/resolved.jsonl
            out/daddylive/resolved.csv
          if-no-files-found: warn

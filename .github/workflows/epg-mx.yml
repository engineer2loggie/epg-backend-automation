name: EPG-MX

on:
  workflow_dispatch: {}
  schedule:
    - cron: "0 7 * * *" # daily 07:00 UTC

concurrency:
  group: epg-mx
  cancel-in-progress: true

jobs:
  mx:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    defaults:
      run:
        shell: bash

    env:
      # GatoTV (EPG only)
      GATOTV_DIR_URL: "https://www.gatotv.com/canales_de_tv"
      GATOTV_TZ: "America/Mexico_City"
      PROGRAMS_HOURS_AHEAD: "24"
      GATOTV_MAX_CHANNELS: "0"     # 0 = all directory channels
      HEADLESS: "true"
      PER_PAGE_DELAY_MS: "200"
      NAV_TIMEOUT_MS: "25000"

      # DB
      SUPABASE_SCHEMA: "public"
      PROGRAMS_TABLE: "epg_programs"
      SUPABASE_URL: "${{ secrets.SUPABASE_URL }}"
      SUPABASE_SERVICE_KEY: "${{ secrets.SUPABASE_SERVICE_KEY }}"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install deps (Playwright + libs)
        run: |
          set -euo pipefail
          npm i --no-save playwright @supabase/supabase-js luxon
          npx playwright install --with-deps chromium

      - name: Write GatoTV EPG 24h scraper
        run: |
          set -euo pipefail
          mkdir -p scripts out/mx
          cat > scripts/mx-gatotv-all.mjs <<'EOF'
          import fs from 'node:fs/promises';
          import path from 'node:path';
          import { chromium } from 'playwright';
          import { DateTime } from 'luxon';
          import { createClient } from '@supabase/supabase-js';

          const GATOTV_DIR_URL = process.env.GATOTV_DIR_URL || 'https://www.gatotv.com/canales_de_tv';
          const GATOTV_TZ = process.env.GATOTV_TZ || 'America/Mexico_City';
          const PROGRAMS_HOURS_AHEAD = Number(process.env.PROGRAMS_HOURS_AHEAD || '24');
          const GATOTV_MAX_CHANNELS = Number(process.env.GATOTV_MAX_CHANNELS || '0');
          const HEADLESS = (process.env.HEADLESS ?? 'true') !== 'false';
          const PER_PAGE_DELAY_MS = Number(process.env.PER_PAGE_DELAY_MS || '200');
          const NAV_TIMEOUT_MS = Number(process.env.NAV_TIMEOUT_MS || '25000');

          const SUPABASE_URL = process.env.SUPABASE_URL || '';
          const SUPABASE_SERVICE_KEY = process.env.SUPABASE_SERVICE_KEY || '';
          const SUPABASE_SCHEMA = process.env.SUPABASE_SCHEMA || 'public';
          const PROGRAMS_TABLE = process.env.PROGRAMS_TABLE || 'epg_programs';

          // ---- helpers ----
          const sleep = (ms)=> new Promise(r=>setTimeout(r,ms));
          const norm = (s)=> String(s||'').replace(/\s+/g,' ').trim();
          const lower = (s)=> norm(s).toLowerCase();

          function parseTimeLocalToUTC(localDateISO, timeStr, tz) {
            // Accept: "07:00", "7:00", "07:00 hrs", "7:00 h", "7:00 am", "19:30"
            let s = lower(timeStr).replace(/\./g,'').replace(/\bhrs?\b/g,'').replace(/\bh\b/g,'').trim();
            const m = s.match(/(\d{1,2}):(\d{2})/);
            if (!m) return null;
            let hh = +m[1], mm = +m[2];
            if (/\b(am|pm)\b/.test(s)) {
              const pm = /\bpm\b/.test(s);
              if (pm && hh < 12) hh += 12;
              if (!pm && hh === 12) hh = 0;
            }
            const t = `${String(hh).padStart(2,'0')}:${String(mm).padStart(2,'0')}:00`;
            return DateTime.fromISO(`${localDateISO}T${t}`, { zone: tz }).toUTC().toISO();
          }

          function clamp24h(programs, nowUTC, hours) {
            const end = nowUTC.plus({ hours });
            return programs
              .filter(p => DateTime.fromISO(p.start_ts) < end)
              .map(p => ({ ...p, stop_ts: DateTime.fromISO(p.stop_ts) > end ? end.toISO() : p.stop_ts }));
          }

          // ---- DB ----
          function sb() {
            return createClient(SUPABASE_URL, SUPABASE_SERVICE_KEY, {
              auth: { persistSession: false }, db: { schema: SUPABASE_SCHEMA },
            });
          }

          async function savePrograms(rows) {
            if (!rows.length) return;
            if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) { console.log('No Supabase creds; skip'); return; }
            const client = sb();
            const B = 500;
            for (let i=0;i<rows.length;i+=B) {
              const slice = rows.slice(i,i+B);
              let { error } = await client.from(PROGRAMS_TABLE).upsert(slice, { onConflict: 'channel_id, start_ts' });
              if (error && /no unique|no exclusion/i.test(error.message||'')) {
                ({ error } = await client.from(PROGRAMS_TABLE).insert(slice));
              }
              if (error) { console.warn('Programs upsert failed:', error.message); break; }
            }
            console.log(`Program ingest attempted: ${rows.length}`);
          }

          // ---- scraping (Playwright, DOM-based) ----
          async function getDirectoryLinks(page) {
            await page.goto(GATOTV_DIR_URL, { waitUntil: 'domcontentloaded', timeout: NAV_TIMEOUT_MS });
            await page.waitForTimeout(400);
            // Collect every anchor to /canal/..., de-dup by absolute URL
            const links = await page.$$eval('a[href*="/canal/"]', as => {
              const seen = new Set(), out = [];
              for (const a of as) {
                const href = a.getAttribute('href') || '';
                if (!href.includes('/canal/')) continue;
                let url;
                try { url = new URL(href, location.href).href; } catch { continue; }
                if (seen.has(url)) continue;
                const name = (a.textContent || '').replace(/\s+/g,' ').trim();
                if (!name) continue;
                seen.add(url);
                out.push({ name, url });
              }
              return out;
            });
            return links;
          }

          // Find header indices by header text
          function mapHeaderIndices(headers) {
            // headers: array of lowercased header cell texts
            const idx = { start: -1, end: -1, title: -1 };
            for (let i=0;i<headers.length;i++) {
              const h = headers[i];
              if (h.includes('hora inicio') || h.includes('inicio')) idx.start = i;
              else if (h.includes('hora fin') || h.includes('fin') || h.includes('término')) idx.end = i;
              else if (h.includes('programa') || h.includes('título') || h.includes('titulo')) idx.title = i;
            }
            // Fallbacks if not found
            if (idx.start === -1 && headers.length >= 1) idx.start = 0;
            if (idx.end === -1 && headers.length >= 2) idx.end = 1;
            if (idx.title === -1 && headers.length >= 3) idx.title = 2;
            return idx;
          }

          async function scrapeChannelPage(page, url) {
            await page.goto(url, { waitUntil: 'domcontentloaded', timeout: NAV_TIMEOUT_MS });
            await page.waitForTimeout(200);

            // Try to locate a table with headers including "Hora Inicio" etc.
            // Strategy:
            //  - find all tables
            //  - for each, read first header row (th/td) texts
            //  - pick the one where at least one header contains "hora" or "programa"
            const tableInfo = await page.evaluate(() => {
              const norm = s => String(s||'').replace(/\s+/g,' ').trim().toLowerCase();
              const tables = Array.from(document.querySelectorAll('table'));
              const out = [];
              for (const tbl of tables) {
                // header cells
                let headerCells = Array.from(tbl.querySelectorAll('thead tr th'));
                if (!headerCells.length) headerCells = Array.from(tbl.querySelectorAll('tr th'));
                if (!headerCells.length) headerCells = Array.from(tbl.querySelectorAll('tr:first-child td'));
                const headers = headerCells.map(th => norm(th.textContent));
                if (!headers.length) continue;
                const joined = headers.join(' ');
                if (!(joined.includes('hora') || joined.includes('programa') || joined.includes('título') || joined.includes('titulo'))) continue;

                // body rows
                const bodyRows = Array.from(tbl.querySelectorAll('tbody tr'));
                const allRows = bodyRows.length ? bodyRows : Array.from(tbl.querySelectorAll('tr')).slice(1);
                const rows = allRows.map(tr => Array.from(tr.querySelectorAll('td')).map(td => (td.textContent||'').replace(/\s+/g,' ').trim()));
                out.push({ headers, rows });
              }
              return out;
            });

            if (!tableInfo.length) return []; // no schedule-looking tables

            // Choose the table with the most data rows
            tableInfo.sort((a,b) => b.rows.length - a.rows.length);
            const chosen = tableInfo[0];

            const idx = mapHeaderIndices(chosen.headers);
            const rows = [];
            for (const r of chosen.rows) {
              if (!r.length) continue;
              const startLocal = r[idx.start] || '';
              const stopLocal  = r[idx.end]   || '';
              const title      = r[idx.title] || '';
              if (!title || !startLocal) continue;
              rows.push({ startLocal, stopLocal: stopLocal || null, title });
            }
            return rows;
          }

          function materializeDay(rows, localISO, tz) {
            const out = [];
            for (let i=0;i<rows.length;i++) {
              const r = rows[i];
              const s = parseTimeLocalToUTC(localISO, r.startLocal, tz);
              if (!s) continue;
              let e = null;
              if (r.stopLocal) {
                e = parseTimeLocalToUTC(localISO, r.stopLocal, tz);
              } else if (rows[i+1]?.startLocal) {
                e = parseTimeLocalToUTC(localISO, rows[i+1].startLocal, tz);
              }
              if (!e) e = DateTime.fromISO(s).plus({ minutes: 60 }).toISO();
              if (DateTime.fromISO(e) <= DateTime.fromISO(s)) {
                e = DateTime.fromISO(s).plus({ minutes: 30 }).toISO();
              }
              out.push({ title: r.title, start_ts: s, stop_ts: e });
            }
            return out;
          }

          async function main() {
            await fs.mkdir('out/mx', { recursive: true });

            const browser = await chromium.launch({
              headless: HEADLESS,
              args: ['--no-sandbox', '--disable-dev-shm-usage'],
            });
            const page = await browser.newPage();
            await page.setExtraHTTPHeaders({
              'User-Agent': 'Mozilla/5.0 (compatible; GatoTV-EPG/1.0)',
              'Accept-Language': 'es-MX,es;q=0.9,en;q=0.5'
            });

            try {
              // 1) Directory → channel links
              let channels = await getDirectoryLinks(page);
              if (GATOTV_MAX_CHANNELS > 0 && channels.length > GATOTV_MAX_CHANNELS) {
                channels = channels.slice(0, GATOTV_MAX_CHANNELS);
              }
              await fs.writeFile(path.join('out','mx','gatotv_directory.json'), JSON.stringify(channels,null,2),'utf8');
              console.log(`GatoTV directory channels: ${channels.length}`);

              // 2) Build 24h window
              const nowUTC = DateTime.utc();
              const localNow = nowUTC.setZone(GATOTV_TZ);
              const todayISO = localNow.toISODate();
              const tomorrowISO = localNow.plus({ days: 1 }).toISODate();

              const programs = [];
              let debugSaved = 0;

              // 3) Sequentially: open, scrape, return, repeat
              for (const ch of channels) {
                // Prefer base page (today). If the site supports dated pages, you can optionally add a second pass
                const rowsToday = await scrapeChannelPage(page, ch.url);
                let mat = materializeDay(rowsToday, todayISO, GATOTV_TZ);

                // If we didn’t reach 24h horizon, try a light pass on tomorrow’s page (if supported)
                let rowsTomorrow = [];
                const dated = ch.url.endsWith(`/${todayISO}`) ? ch.url.replace(`/${todayISO}`, `/${tomorrowISO}`) : `${ch.url.replace(/\/$/,'')}/${tomorrowISO}`;
                try { rowsTomorrow = await scrapeChannelPage(page, dated); } catch { rowsTomorrow = []; }
                const mat2 = materializeDay(rowsTomorrow, tomorrowISO, GATOTV_TZ);

                const merged = clamp24h([...mat, ...mat2], nowUTC, PROGRAMS_HOURS_AHEAD);

                if (!merged.length && debugSaved < 3) {
                  // Save raw HTML once or twice for triage
                  try {
                    const html = await page.content();
                    await fs.writeFile(path.join('out','mx',`debug_${debugSaved+1}.html`), html, 'utf8');
                    debugSaved++;
                  } catch {}
                }

                for (const r of merged) {
                  programs.push({
                    channel_id: ch.url,  // using the GatoTV page URL as the channel identifier for this test
                    start_ts: r.start_ts,
                    stop_ts: r.stop_ts,
                    title: r.title,
                    sub_title: null,
                    summary: null,
                    categories: [],
                    program_url: null,
                    episode_num_xmltv: null,
                    icon_url: null,
                    rating: null,
                    star_rating: null,
                    season: null,
                    episode: null,
                    language: 'es',
                    orig_language: 'es',
                    credits: null,
                    premiere: false,
                    previously_shown: false,
                    extras: { source: 'gatotv', channel_name: ch.name },
                    ingested_at: DateTime.utc().toISO()
                  });
                }

                await sleep(PER_PAGE_DELAY_MS);
              }

              await fs.writeFile(path.join('out','mx','epg_programs_sample.json'), JSON.stringify(programs.slice(0,200),null,2),'utf8');
              await savePrograms(programs);
              console.log(`GatoTV 24h ingest complete. Channels scraped: ${channels.length}, programs: ${programs.length}`);
            } finally {
              await browser.close();
            }
          }

          main().catch(e => { console.error(e); process.exit(1); });
          EOF

      - name: Run GatoTV-only 24h ingest
        run: |
          set -euo pipefail
          node scripts/mx-gatotv-all.mjs

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: mx-output
          path: |
            out/mx/gatotv_directory.json
            out/mx/epg_programs_sample.json
            out/mx/debug_*.html
          if-no-files-found: warn

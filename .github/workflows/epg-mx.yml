name: EPG-MX

on:
  workflow_dispatch: {}
  schedule:
    - cron: "0 7 * * *" # daily 07:00 UTC

concurrency:
  group: epg-mx
  cancel-in-progress: true

jobs:
  mx:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    defaults:
      run:
        shell: bash

    env:
      # Step A: iptv-org discovery/probing
      MX_SEARCH_URL: "https://iptv-org.github.io/?q=live%20country:MX"
      HEADLESS: "true"
      MAX_CHANNELS: "0"            # 0 = no cap
      PER_PAGE_DELAY_MS: "150"
      NAV_TIMEOUT_MS: "30000"

      # PROBE tuning (no VPN)
      PROBE_TIMEOUT_MS: "10000"    # 10s as requested
      PROBE_CONCURRENCY: "10"      # be nice to CDNs; raise carefully
      PROBE_REFERER: "https://iptv-org.github.io/"  # fallback Referer if we don't know the channel page (we do pass real page when we have it)
      # Optional: playlist to enrich tvg-id by url/name (not required)
      M3U_URL: "${{ secrets.M3U_URL }}"

      # Step B: GatoTV full crawl (no matching here)
      GATOTV_DIR_URL: "https://www.gatotv.com/canales_de_tv"
      GATOTV_TZ: "America/Mexico_City"
      PROGRAMS_HOURS_AHEAD: "24"
      GATOTV_MAX_CHANNELS: "0"     # 0 = all directory channels

      # DB
      SUPABASE_SCHEMA: "public"
      STREAMS_TABLE: "mx_working_streams"
      PROGRAMS_TABLE: "epg_programs"
      SUPABASE_URL: "${{ secrets.SUPABASE_URL }}"
      SUPABASE_SERVICE_KEY: "${{ secrets.SUPABASE_SERVICE_KEY }}"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install Node deps (Playwright + libs)
        run: |
          set -euo pipefail
          npm i --no-save playwright @supabase/supabase-js luxon
          npx playwright install --with-deps chromium

      - name: Write Script A – Discover/Probe iptv-org (.m3u8) and save
        run: |
          set -euo pipefail
          mkdir -p scripts out/mx
          cat > scripts/mx-discover-streams.mjs <<'EOF'
          import { chromium } from 'playwright';
          import fs from 'node:fs/promises';
          import path from 'node:path';
          import { setTimeout as delay } from 'node:timers/promises';
          import { createClient } from '@supabase/supabase-js';

          const SEARCH_URL = process.env.MX_SEARCH_URL || 'https://iptv-org.github.io/?q=live%20country:MX';
          const HEADLESS = (process.env.HEADLESS ?? 'true') !== 'false';
          const MAX_CHANNELS = Number(process.env.MAX_CHANNELS || '0');
          const PER_PAGE_DELAY_MS = Number(process.env.PER_PAGE_DELAY_MS || '150');
          const NAV_TIMEOUT_MS = Number(process.env.NAV_TIMEOUT_MS || '30000');

          const PROBE_TIMEOUT_MS = Number(process.env.PROBE_TIMEOUT_MS || '10000');
          const PROBE_CONCURRENCY = Number(process.env.PROBE_CONCURRENCY || '10');
          const PROBE_REFERER = (process.env.PROBE_REFERER || '').trim();

          const SUPABASE_URL = process.env.SUPABASE_URL || '';
          const SUPABASE_SERVICE_KEY = process.env.SUPABASE_SERVICE_KEY || '';
          const SUPABASE_SCHEMA = process.env.SUPABASE_SCHEMA || 'public';
          const STREAMS_TABLE = process.env.STREAMS_TABLE || 'mx_working_streams';

          const M3U_URL = (process.env.M3U_URL || '').trim();

          function stripAccents(s){return String(s).normalize('NFD').replace(/\p{Diacritic}+/gu,'');}
          const STOP=new Set(['canal','tv','television','hd','sd','mx','mexico','méxico','el','la','los','las','de','del','y','en','the','channel']);
          function tokensOf(s){ if(!s) return []; let p=stripAccents(String(s).toLowerCase()).replace(/&/g,' and ').replace(/[^a-z0-9]+/g,' ').trim(); return p.split(/\s+/).filter(t=>t && !STOP.has(t));}
          function keyOf(s){ return Array.from(new Set(tokensOf(s))).sort().join(' '); }

          function parseM3U(text){
            const items=[]; let cur=null;
            for (const raw of text.split(/\r?\n/)) {
              const line=raw.trim(); if(!line) continue;
              if (line.startsWith('#EXTINF')) {
                const attrs={};
                for (const m of line.matchAll(/\b([a-z0-9_-]+)="([^"]*)"/gi)) attrs[m[1].toLowerCase()]=m[2];
                const comma=line.indexOf(','); const title=comma>=0?line.slice(comma+1).trim():'';
                cur = { tvg_id: attrs['tvg-id']||null, tvg_name: attrs['tvg-name']||title||null, url:null };
              } else if (!line.startsWith('#') && cur) { cur.url=line; items.push(cur); cur=null; }
            }
            return items;
          }
          async function buildM3U(){
            const out={ byUrl:new Map(), byNameKey:new Map() };
            if(!M3U_URL) return out;
            try{
              const txt=await (await fetch(M3U_URL)).text();
              const items=parseM3U(txt);
              for (const it of items) if (it.url && it.tvg_id) out.byUrl.set(it.url, it.tvg_id);
              const seen=new Set();
              for (const it of items) {
                const k=keyOf(it.tvg_name || '');
                if (k && it.tvg_id && !seen.has(k)) { out.byNameKey.set(k, it.tvg_id); seen.add(k); }
              }
              console.log(`M3U parsed: ${items.length} entries`);
            } catch(e){ console.warn(`M3U parse skipped: ${e.message}`); }
            return out;
          }

          async function collectChannelPages(browser){
            const page = await browser.newPage();
            page.setDefaultTimeout(NAV_TIMEOUT_MS);
            await page.goto(SEARCH_URL, { waitUntil: 'domcontentloaded' });
            await page.waitForSelector('a[href*="/channels/"]', { timeout: 15000 }).catch(()=>{});
            await page.waitForTimeout(600);

            let items = await page.$$eval('a[href*="/channels/"]', as => {
              const out = [];
              for (const a of as) {
                const href = a.getAttribute('href') || '';
                if (!href.includes('/channels/')) continue;
                const url = new URL(href, location.href).href;
                const name = (a.textContent || '').trim();
                out.push({ url, name });
              }
              const m = new Map();
              for (const it of out) if (!m.has(it.url)) m.set(it.url, it);
              return [...m.values()];
            });

            items = items.filter(i => i.name && i.url);
            if (MAX_CHANNELS > 0 && items.length > MAX_CHANNELS) items = items.slice(0, MAX_CHANNELS);
            await page.close();
            return items.map(i => ({ ...i, nameKey: keyOf(i.name) }));
          }

          async function scrapeChannel(browser, link){
            const page = await browser.newPage();
            page.setDefaultTimeout(NAV_TIMEOUT_MS);
            try{
              await page.goto(link.url, { waitUntil: 'domcontentloaded' });
              await page.waitForTimeout(400);

              // Basic logo (best-effort)
              const logoUrl = await page.evaluate(()=>{
                const og = document.querySelector('meta[property="og:image"]');
                if (og && og.content) return og.content;
                const img = document.querySelector('img');
                return img ? img.src : null;
              });

              // Streams tab (if present)
              const tab = await page.$('text=Streams');
              if (tab) { await tab.click().catch(()=>{}); await page.waitForTimeout(250); }

              let anchors = await page.$$eval('a[href*=".m3u8"]', els => els.map(e => e.href));
              if (!anchors.length) {
                const html = await page.content();
                const rx = /https?:\/\/[^\s"'<>]+\.m3u8[^\s"'<>]*/gi;
                const set = new Set(); let m; while ((m = rx.exec(html))) set.add(m[0]);
                anchors = [...set];
              }
              anchors = [...new Set(anchors)].filter(u => /^https?:\/\//i.test(u));

              return { channelName: link.name, channelUrl: link.url, logoUrl: logoUrl || null, streams: anchors.map(url => ({ url })) };
            } catch(e){
              console.warn(`Error scraping ${link.url}: ${e.message}`);
              return { channelName: link.name, channelUrl: link.url, logoUrl: null, streams: [] };
            } finally {
              await page.close();
              await delay(PER_PAGE_DELAY_MS);
            }
          }

          function buildProbeHeaders(channelPageUrl){
            const ref = channelPageUrl || PROBE_REFERER || undefined;
            let origin;
            try { origin = ref ? new URL(ref).origin : undefined; } catch { origin = undefined; }
            const h = {
              'user-agent': 'Mozilla/5.0',
              'accept': 'application/vnd.apple.mpegurl,text/plain,*/*'
            };
            if (ref) h['referer'] = ref;
            if (origin) h['origin'] = origin;
            return h;
          }

          async function head(url, headers, signal){
            try {
              const r = await fetch(url, { method: 'HEAD', headers, redirect: 'follow', signal });
              return { ok: r.ok, status: r.status, ct: r.headers.get('content-type')||'', redirected: r.redirected, finalUrl: r.url };
            } catch (e) { return { ok:false, status:0, error:String(e) }; }
          }
          async function getRange(url, headers, signal){
            try {
              const r = await fetch(url, { method: 'GET', headers: { ...headers, Range: 'bytes=0-1023' }, redirect: 'follow', signal });
              return { ok: r.status===200 || r.status===206, status: r.status, ct: r.headers.get('content-type')||'', redirected: r.redirected, finalUrl: r.url };
            } catch (e) { return { ok:false, status:0, error:String(e) }; }
          }
          async function get200(url, headers, signal){
            try {
              const r = await fetch(url, { method: 'GET', headers, redirect: 'follow', signal });
              return { ok: r.status===200, status: r.status, ct: r.headers.get('content-type')||'', redirected: r.redirected, finalUrl: r.url };
            } catch (e) { return { ok:false, status:0, error:String(e) }; }
          }

          async function probeOne(url, channelPageUrl){
            const ac = new AbortController();
            const to = setTimeout(()=>ac.abort(), PROBE_TIMEOUT_MS);
            const headers = buildProbeHeaders(channelPageUrl);

            // 1) HEAD
            let res = await head(url, headers, ac.signal);
            if (!res.ok) {
              // 2) GET with Range
              res = await getRange(url, headers, ac.signal);
            }
            if (!res.ok) {
              // 3) plain GET 200 check
              res = await get200(url, headers, ac.signal);
            }

            clearTimeout(to);
            return { url, ok: !!res.ok, status: res.status, contentType: res.ct, redirected: res.redirected, finalUrl: res.finalUrl, error: res.error || null };
          }

          async function pLimit(n, items, fn){
            const ret = new Array(items.length);
            let i=0; const workers = Array.from({length:n}, async ()=>{
              while (i < items.length) {
                const idx = i++;
                ret[idx] = await fn(items[idx], idx);
              }
            });
            await Promise.all(workers);
            return ret;
          }

          function sb(){ return createClient(SUPABASE_URL, SUPABASE_SERVICE_KEY, { auth:{persistSession:false}, db:{schema:SUPABASE_SCHEMA} }); }

          async function main(){
            await fs.mkdir('out/mx', { recursive: true });
            const browser = await chromium.launch({ headless: HEADLESS });
            const m3u = await buildM3U();

            try{
              const links = await collectChannelPages(browser);
              console.log(`Found ${links.length} channel pages.`);
              const scraped = await Promise.all(links.map(l => scrapeChannel(browser, l)));

              const records = [];
              const probes = [];
              const urls = [];
              const byUrlMeta = new Map();

              for (const ch of scraped) {
                for (const s of ch.streams) {
                  if (!byUrlMeta.has(s.url)) {
                    byUrlMeta.set(s.url, { channelName: ch.channelName, channelUrl: ch.channelUrl, logoUrl: ch.logoUrl });
                    urls.push(s.url);
                  }
                }
              }

              console.log(`Probing ${urls.length} unique streams with concurrency ${PROBE_CONCURRENCY}...`);

              const probeRes = await pLimit(PROBE_CONCURRENCY, urls, async (u) => {
                const meta = byUrlMeta.get(u);
                const pr = await probeOne(u, meta?.channelUrl);
                return pr;
              });

              await fs.writeFile(path.join('out','mx','probe_results.json'), JSON.stringify(probeRes, null, 2), 'utf8');

              const working = new Set(probeRes.filter(x => x.ok).map(x => x.url));
              console.log(`Working streams: ${working.size}`);

              for (const u of urls) {
                const meta = byUrlMeta.get(u);
                const tvgFromUrl = m3u.byUrl.get(u) || null;
                const tvgFromName = m3u.byNameKey.get(keyOf(meta.channelName)) || null;
                if (working.has(u)) {
                  records.push({
                    stream_url: u,
                    channel_id: tvgFromUrl || tvgFromName || null,
                    channel_name: meta.channelName,
                    logo_url: meta.logoUrl || null,
                    source_page_url: meta.channelUrl,
                    quality: null,
                    rank: 1,
                    extras: null,
                    working: true,
                    checked_at: new Date().toISOString()
                  });
                }
              }

              await fs.writeFile(path.join('out','mx','working_channels.json'), JSON.stringify(records, null, 2), 'utf8');

              if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) { console.log('No Supabase creds; skipping DB upsert'); return; }
              if (!records.length) { console.log('No working streams to save.'); return; }

              const client = sb(); const BATCH=500;
              for (let i=0;i<records.length;i+=BATCH){
                const slice = records.slice(i,i+BATCH);
                const { error } = await client.from(STREAMS_TABLE).upsert(slice, { onConflict: 'stream_url' });
                if (error) { console.warn(`Upsert batch failed: ${error.message}`); break; }
              }
              console.log(`Saved ${records.length} working streams to ${STREAMS_TABLE}`);
            } finally {
              await browser.close();
            }
          }

          main().catch(e => { console.error(e); process.exit(1); });
          EOF

      - name: Run Script A – Discover & save working streams (no VPN)
        run: |
          set -euo pipefail
          node scripts/mx-discover-streams.mjs

      - name: Write Script B – Crawl ALL GatoTV channels and ingest 24h
        run: |
          set -euo pipefail
          cat > scripts/mx-gatotv-all.mjs <<'EOF'
          import fs from 'node:fs/promises';
          import path from 'node:path';
          import { createClient } from '@supabase/supabase-js';
          import { DateTime } from 'luxon';

          const GATOTV_DIR_URL = process.env.GATOTV_DIR_URL || 'https://www.gatotv.com/canales_de_tv';
          const GATOTV_TZ = process.env.GATOTV_TZ || 'America/Mexico_City';
          const PROGRAMS_HOURS_AHEAD = Number(process.env.PROGRAMS_HOURS_AHEAD || '24');
          const GATOTV_MAX_CHANNELS = Number(process.env.GATOTV_MAX_CHANNELS || '0'); // 0 = no cap

          const SUPABASE_URL = process.env.SUPABASE_URL || '';
          const SUPABASE_SERVICE_KEY = process.env.SUPABASE_SERVICE_KEY || '';
          const SUPABASE_SCHEMA = process.env.SUPABASE_SCHEMA || 'public';
          const PROGRAMS_TABLE = process.env.PROGRAMS_TABLE || 'epg_programs';

          function cleanText(html){return String(html).replace(/<[^>]+>/g,' ').replace(/\s+/g,' ').trim();}

          async function fetchHtml(url){
            const r = await fetch(url, { headers: { 'user-agent': 'Mozilla/5.0' } });
            if(!r.ok) throw new Error(`HTTP ${r.status} for ${url}`);
            return await r.text();
          }

          function extractDirectoryChannels(html){
            const out = [];
            const re = /<a\\s+[^>]*href=["'](\\/canal\\/[^"'#?]+)["'][^>]*>([\\s\\S]*?)<\\/a>/gi;
            const seen = new Set();
            let m;
            while((m=re.exec(html))){
              const rel = m[1];
              const name = cleanText(m[2]);
              const url = new URL(rel, GATOTV_DIR_URL).href;
              if(!name || seen.has(url)) continue;
              seen.add(url);
              out.push({ name, url });
            }
            return out;
          }

          function parseSchedule(html){
            const rows=[]; 
            const cleaned=html.replace(/\\r|\\n/g,' ').replace(/<\\s*br\\s*\\/?>(?=\\S)/gi,' ').replace(/\\s+/g,' ');
            const rx=/(\\b\\d{1,2}:\\d{2}\\s*(?:a\\.?m\\.?|p\\.?m\\.)?)\\s*-?\\s*(\\b\\d{1,2}:\\d{2}\\s*(?:a\\.?m\\.?|p\\.?m\\.)?)?[^>]*?<[^>]*?>([^<]{2,200})/gi;
            const seen=new Set(); let m;
            while((m=rx.exec(cleaned))){
              const start=m[1]?.trim(); const stop=(m[2]||'').trim()||null; const title=(m[3]||'').trim();
              const key=\`\${start}|\${stop}|\${title}\`;
              if(!title || !start || seen.has(key)) continue;
              seen.add(key); rows.push({ startLocal:start, stopLocal:stop, title });
            }
            return rows;
          }

          function parseLocal(dateISO, timeStr, tz){
            let s=(timeStr||'').toLowerCase().replace(/\\./g,'').replace(/\\s+/g,' ');
            const m=s.match(/(\\d{1,2}):(\\d{2})/); if(!m) return null;
            let h=+m[1], mi=+m[2]; const ampm=/(am|pm)\\b/.test(s); const isPM=/pm\\b/.test(s);
            if(ampm){ if(isPM && h<12) h+=12; if(!isPM && h===12) h=0; }
            return DateTime.fromISO(\`\${dateISO}T\${String(h).padStart(2,'0')}:\${String(mi).padStart(2,'0')}:00\`,{zone:tz}).toUTC().toISO();
          }

          function materializeDay(rows, localISO, tz){
            const out=[]; 
            for(let i=0;i<rows.length;i++){
              const r=rows[i]; const start=parseLocal(localISO, r.startLocal, tz); if(!start) continue;
              let stop=null;
              if(r.stopLocal) stop=parseLocal(localISO, r.stopLocal, tz);
              else if(rows[i+1]?.startLocal) stop=parseLocal(localISO, rows[i+1].startLocal, tz);
              if(!stop) stop=DateTime.fromISO(start).plus({minutes:60}).toISO();
              if(DateTime.fromISO(stop) <= DateTime.fromISO(start)) stop=DateTime.fromISO(start).plus({minutes:30}).toISO();
              out.push({ title:r.title, start_ts:start, stop_ts:stop });
            }
            return out;
          }

          async function fetchChannelDay(url, localISO){
            let u=url; if(!/(\\/)\\d{4}-\\d{2}-\\d{2}$/.test(url)) u=url.replace(/\\/?$/, \`/\${localISO}\`);
            try{
              const html = await fetchHtml(u);
              return { url:u, rows:parseSchedule(html) };
            }catch{ return { url:u, rows:[] }; }
          }

          function clamp24h(programs, nowUTC, hours){
            const end=nowUTC.plus({hours});
            return programs
              .filter(p => DateTime.fromISO(p.start_ts) < end)
              .map(p => ({ ...p, stop_ts: DateTime.fromISO(p.stop_ts) > end ? end.toISO() : p.stop_ts }));
          }

          function sb(){ return createClient(SUPABASE_URL, SUPABASE_SERVICE_KEY, { auth:{persistSession:false}, db:{schema:SUPABASE_SCHEMA} }); }

          async function savePrograms(programs){
            if(!programs.length) return;
            if(!SUPABASE_URL || !SUPABASE_SERVICE_KEY){ console.log('No Supabase creds; skip'); return; }
            const client=sb(); const BATCH=500;
            for(let i=0;i<programs.length;i+=BATCH){
              const slice=programs.slice(i,i+BATCH);
              let { error } = await client.from(PROGRAMS_TABLE).upsert(slice,{ onConflict:'channel_id, start_ts' });
              if(error && /no unique|no exclusion/i.test(error.message||'')) ({ error } = await client.from(PROGRAMS_TABLE).insert(slice));
              if(error){ console.warn(\`Programs batch failed: \${error.message}\`); break; }
            }
            console.log(\`Program ingest attempted: \${programs.length}\`);
          }

          async function main(){
            await fs.mkdir('out/mx',{recursive:true});

            // 1) Directory → channel links
            const dirHtml = await fetchHtml(GATOTV_DIR_URL);
            let channels = extractDirectoryChannels(dirHtml);
            if(GATOTV_MAX_CHANNELS>0 && channels.length>GATOTV_MAX_CHANNELS) channels = channels.slice(0, GATOTV_MAX_CHANNELS);
            await fs.writeFile(path.join('out','mx','gatotv_directory.json'), JSON.stringify(channels,null,2),'utf8');
            console.log(\`GatoTV directory channels: \${channels.length}\`);

            // 2) Today + tomorrow → clamp to 24h
            const nowUTC = DateTime.utc();
            const localNow = nowUTC.setZone(GATOTV_TZ);
            const todayISO = localNow.toISODate();
            const tomorrowISO = localNow.plus({days:1}).toISODate();

            const programs=[];
            for(const ch of channels){
              const d1 = await fetchChannelDay(ch.url, todayISO);
              const d2 = await fetchChannelDay(ch.url, tomorrowISO);
              const a1 = materializeDay(d1.rows, todayISO, GATOTV_TZ);
              const a2 = materializeDay(d2.rows, tomorrowISO, GATOTV_TZ);
              const clamped = clamp24h(a1.concat(a2), nowUTC, PROGRAMS_HOURS_AHEAD);

              for(const r of clamped){
                programs.push({
                  channel_id: ch.url,       // use GatoTV URL as ID (you'll join in SQL)
                  start_ts: r.start_ts,
                  stop_ts: r.stop_ts,
                  title: r.title,
                  sub_title: null,
                  summary: null,
                  categories: [],
                  program_url: null,
                  episode_num_xmltv: null,
                  icon_url: null,
                  rating: null,
                  star_rating: null,
                  season: null,
                  episode: null,
                  language: 'es',
                  orig_language: 'es',
                  credits: null,
                  premiere: false,
                  previously_shown: false,
                  extras: { source: 'gatotv', channel_name: ch.name },
                  ingested_at: DateTime.utc().toISO(),
                  source_epg: 'gatotv',
                  source_url: ch.url
                });
              }
            }

            await fs.writeFile(path.join('out','mx','epg_programs_sample.json'), JSON.stringify(programs.slice(0,200),null,2),'utf8');
            await savePrograms(programs);
            console.log(\`GatoTV 24h ingest complete. Channels scraped: \${channels.length}, programs: \${programs.length}\`);
          }

          main().catch(e=>{ console.error(e); process.exit(1); });
          EOF

      - name: Run Script B – GatoTV 24h ingest (no matching here)
        run: |
          set -euo pipefail
          node scripts/mx-gatotv-all.mjs

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: mx-output
          path: |
            out/mx/working_channels.json
            out/mx/probe_results.json
            out/mx/gatotv_directory.json
            out/mx/epg_programs_sample.json
          if-no-files-found: warn
